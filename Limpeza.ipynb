{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importação de bibliotecas."
      ],
      "metadata": {
        "id": "WpNPzXXQ1b-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry_fL6gGDG4p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurações de exibição do pandas"
      ],
      "metadata": {
        "id": "el1ZHI3G7vsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None) # exibir todas as colunas\n",
        "pd.set_option('display.width', 1000)      # aumentar a largura da exibição para evitar quebras de linha indesejadas"
      ],
      "metadata": {
        "id": "HIbVnmU970fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C2nLYKnQ70Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando um arquivo CSV a partir de um caminho especificado."
      ],
      "metadata": {
        "id": "qnzhMzye3q9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_dados(caminho_arquivo: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(caminho_arquivo)\n",
        "        if df.duplicated().any():\n",
        "            print(f\"aviso: encontradas e removidas {df.duplicated().sum()} linhas duplicadas em '{caminho_arquivo}'.\")\n",
        "            df = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"erro: o arquivo '{caminho_arquivo}' não foi encontrado.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "dbPrs29R3xTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padronizando links de vídeo para um formato consistente. Ex: 'www.youtube.com' vira 'youtube.com', 'youtu.be' vira 'youtube.com/watch?v='."
      ],
      "metadata": {
        "id": "zEUYBvUdVg1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_video_link(url: str) -> str:\n",
        "    if pd.isna(url) or url == -1: # tratar valores NaN ou -1\n",
        "        return url\n",
        "\n",
        "    url_str = str(url).strip().lower() # limpa espaços e padroniza para minúsculas\n",
        "\n",
        "    # normaliza links do twitch\n",
        "    if 'twitch.tv/videos/' in url_str:\n",
        "        return url_str.replace('www.', '').replace('m.', '')\n",
        "\n",
        "    # normaliza links do youtube\n",
        "    if 'youtube.com/watch?v=' in url_str:\n",
        "        return url_str.replace('www.', '') # remove www.\n",
        "    if 'youtu.be/' in url_str: # converte youtu.be para o formato watch?v=\n",
        "        match = re.search(r'youtu\\.be/([a-zA-Z0-9_-]+)', url_str)\n",
        "        if match:\n",
        "            video_id = match.group(1)\n",
        "            return f'https://youtube.com/watch?v={video_id}'\n",
        "        return url_str # retorna original se não conseguir extrair ID\n",
        "\n",
        "    return url_str # retorna a url original se não for twitch/youtube ou já estiver normalizada"
      ],
      "metadata": {
        "id": "zf4AQ5joVp8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processa dados do bilibili para calcular estatísticas de canal\n",
        "5 dias antes e 5 dias depois de cada vídeo de recorde.\n",
        "\n"
      ],
      "metadata": {
        "id": "LfpqYSqUjfLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a ideia é carregar os dados do Bilibili e para cada vídeo de recorde\n",
        "# calcular a média de visualizações e likes nos cinco dias anteriores e posteriores\n",
        "def processar_recordes_bilibili_com_5d(caminho_csv: str) -> pd.DataFrame:\n",
        "    print(f\"\\n--- Processando Bilibili a partir de: {caminho_csv} ---\")\n",
        "\n",
        "   #carregando o csv\n",
        "    try:\n",
        "        df = pd.read_csv(caminho_csv, parse_dates=[\"data_publicacao\"])\n",
        "        print(f\"Bilibili: CSV carregado. {len(df)} linhas iniciais.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Bilibili ERRO: O arquivo '{caminho_csv}' não foi encontrado.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Bilibili ERRO ao carregar CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # garantimos que a coluna de data esteja no formato certo\n",
        "    df[\"data_publicacao\"] = pd.to_datetime(df[\"data_publicacao\"], errors=\"coerce\")\n",
        "    if df[\"data_publicacao\"].isnull().any():\n",
        "        print(\"Bilibili AVISO: Algumas 'data_publicacao' são inválidas e foram convertidas para NaT.\")\n",
        "\n",
        "    # filtrando para pegar apenas os vídeos marcados como recorde\n",
        "    # o tratamento com strip e lower é para evitar problemas com espaços ou maiúsculas\n",
        "    df_recordes = df[\n",
        "        df[\"context_video\"].astype(str).str.strip().str.lower() == \"recorde\"\n",
        "    ].copy()\n",
        "    print(f\"Bilibili: {len(df_recordes)} vídeos com 'context_video' == 'recorde'.\")\n",
        "\n",
        "    # se não encontrarmos nenhum vídeo de recorde não há o que fazer\n",
        "    if df_recordes.empty:\n",
        "        print(\"Bilibili AVISO: Nenhum vídeo de 'recorde' encontrado após a filtragem. Retornando DataFrame vazio.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # aqui vamos guardar os resultados de cada vídeo de recorde processado\n",
        "    resultados = []\n",
        "\n",
        "    # passando por cada vídeo de recorde que encontramos\n",
        "    for index, rec in df_recordes.iterrows():\n",
        "        streamer = rec[\"name_streamer\"]\n",
        "        data_rec = rec[\"data_publicacao\"]\n",
        "\n",
        "        # verificação de segurança caso a data do recorde seja inválida\n",
        "        if pd.isna(data_rec):\n",
        "            print(f\"Bilibili AVISO: Ignorando recorde do streamer '{streamer}' devido a 'data_publicacao' inválida.\")\n",
        "            continue\n",
        "\n",
        "        # selecionando todos os vídeos do mesmo streamer na janela de tempo que nos interessa\n",
        "\n",
        "        antes = df[\n",
        "            (df[\"name_streamer\"] == streamer)\n",
        "            & (df[\"data_publicacao\"] >= data_rec - timedelta(days=5))\n",
        "            & (df[\"data_publicacao\"] < data_rec)\n",
        "        ]\n",
        "        depois = df[\n",
        "            (df[\"name_streamer\"] == streamer)\n",
        "            & (df[\"data_publicacao\"] > data_rec)\n",
        "            & (df[\"data_publicacao\"] <= data_rec + timedelta(days=5))\n",
        "        ]\n",
        "\n",
        "        # calculando as médias para o período antes e depois\n",
        "        # se não houver vídeos em um dos períodos o valor padrão será -1\n",
        "        views_antes = int(antes[\"views\"].mean()) if not antes.empty else -1\n",
        "        likes_antes = int(antes[\"likes\"].mean()) if not antes.empty else -1\n",
        "        views_depois = int(depois[\"views\"].mean()) if not depois.empty else -1\n",
        "        likes_depois = int(depois[\"likes\"].mean()) if not depois.empty else -1\n",
        "\n",
        "        #montando um dicionário com os dados originais do recorde\n",
        "        rec_final = rec.to_dict()\n",
        "        # adicionando as novas colunas com as médias que calculamos\n",
        "        rec_final.update({\n",
        "            \"Views_5d_Antes\": views_antes,\n",
        "            \"Likes_5d_Antes\": likes_antes,\n",
        "            \"Views_5d_Depois\": views_depois,\n",
        "            \"Likes_5d_Depois\": likes_depois\n",
        "        })\n",
        "\n",
        "        # adicionando o resultado completo à nossa lista de resultados\n",
        "        resultados.append(rec_final)\n",
        "\n",
        "    # transformando a lista de dicionários em um novo DataFrame\n",
        "    df_final = pd.DataFrame(resultados)\n",
        "    print(f\"Bilibili: {len(df_final)} recordes processados e adicionados ao DataFrame.\")\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "bvIicRQljkga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando, padronizando e combinando os DataFrames de estatísticas de vídeo, garantindo que todas as colunas do YouTube existam em todos os DataFrames, preenchendo os valores ausentes com -1.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfCDS6_i4-qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unificar_estatisticas_videos(path_twitch: str, path_youtube: str, path_bilibili: str) -> pd.DataFrame:\n",
        "\n",
        "    df_twitch_raw = carregar_dados(path_twitch)\n",
        "    df_youtube_raw = carregar_dados(path_youtube)\n",
        "\n",
        "    processed_dfs = []\n",
        "\n",
        "    # --- processar twitch ---\n",
        "    if not df_twitch_raw.empty:\n",
        "        df_twitch_temp = df_twitch_raw.rename(columns={\n",
        "            'url': 'video_link', 'views': 'video_views', 'title': 'video_titulo'\n",
        "        }).copy()\n",
        "        df_twitch_temp['plataforma'] = 'Twitch'\n",
        "        df_twitch_temp['video_link'] = df_twitch_temp['video_link'].apply(normalizar_video_link)\n",
        "        processed_dfs.append(df_twitch_temp)\n",
        "\n",
        "    # --- processar youtube ---\n",
        "    if not df_youtube_raw.empty:\n",
        "        df_youtube_temp = df_youtube_raw.rename(columns={\n",
        "            'Views': 'video_views',\n",
        "            'Likes': 'video_likes',\n",
        "            'Comments': 'video_comentarios',\n",
        "            'PublishedAt': 'video_data_publicacao',\n",
        "            'title': 'video_titulo'\n",
        "        }).copy()\n",
        "        df_youtube_temp['plataforma'] = 'YouTube'\n",
        "        df_youtube_temp['video_link'] = df_youtube_temp['video_link'].apply(normalizar_video_link)\n",
        "        colunas_redundantes = ['date', 'player', 'time_seconds', 'time_formatted', 'run_link']\n",
        "        df_youtube_temp = df_youtube_temp.drop(columns=[c for c in colunas_redundantes if c in df_youtube_temp.columns], errors='ignore')\n",
        "        processed_dfs.append(df_youtube_temp)\n",
        "\n",
        "    # --- processar bilibili ---\n",
        "    df_bilibili_processed = processar_recordes_bilibili_com_5d(path_bilibili)\n",
        "\n",
        "    if not df_bilibili_processed.empty:\n",
        "        df_bilibili_temp = df_bilibili_processed.rename(columns={\n",
        "            'views': 'video_views',\n",
        "            'likes': 'video_likes',\n",
        "            'comments': 'video_comentarios',\n",
        "            'data_publicacao': 'video_data_publicacao',\n",
        "            'title': 'video_titulo'\n",
        "        }).copy()\n",
        "        df_bilibili_temp['video_link'] = 'https://www.bilibili.com/video/' + df_bilibili_temp['bvid'].astype(str)\n",
        "\n",
        "        df_bilibili_temp['video_link'] = df_bilibili_temp['video_link'].apply(normalizar_video_link)\n",
        "\n",
        "        df_bilibili_temp['plataforma'] = 'Bilibili'\n",
        "        processed_dfs.append(df_bilibili_temp)\n",
        "\n",
        "    # --- unificação ---\n",
        "    if not processed_dfs:\n",
        "        print(\"Aviso: Nenhum DataFrame de plataforma foi carregado corretamente.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_stats_unificado = pd.concat(processed_dfs, ignore_index=True)\n",
        "    df_stats_unificado = df_stats_unificado.fillna(-1)\n",
        "\n",
        "    return df_stats_unificado"
      ],
      "metadata": {
        "id": "3wrZd1tS5DOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Juntando os dados, removendo duplicatas pós-junção e convertendo os tipos de dados."
      ],
      "metadata": {
        "id": "mvSs_Iphz4mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def juntar_e_limpar_dados(path_runs: str, df_stats: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    df_runs = carregar_dados(path_runs)\n",
        "    # normalizar links das runs antes do merge\n",
        "    df_runs['video_link'] = df_runs['video_link'].apply(normalizar_video_link)\n",
        "    df_runs = df_runs.drop_duplicates(subset=['video_link'], keep='first')\n",
        "\n",
        "    df_runs = df_runs.rename(columns={\n",
        "        'date': 'run_date', 'player': 'run_player', 'time_seconds': 'run_time_seconds',\n",
        "        'time_formatted': 'run_time_formatted', 'run_link': 'run_url'\n",
        "    })\n",
        "\n",
        "    # left merge para manter todas as runs, preenchendo o que não encontrar com NaN\n",
        "    df_final = pd.merge(df_runs, df_stats, on='video_link', how='left')\n",
        "\n",
        "    # conversão de tipos de dados para datas\n",
        "    # o errors='coerce' transforma datas inválidas em NaT, que serão preenchidos\n",
        "    df_final['run_date'] = pd.to_datetime(df_final['run_date'], errors='coerce')\n",
        "    df_final['video_data_publicacao'] = pd.to_datetime(df_final['video_data_publicacao'], errors='coerce')\n",
        "\n",
        "    # preencher todos os NaNs remanescentes (do merge e de NaT) com -1\n",
        "    pd.set_option('future.no_silent_downcasting', True)\n",
        "    df_final = df_final.fillna(-1).infer_objects(copy=False)\n",
        "\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "lEXfqavnz9dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva o Dataframe em um arquivo .csv"
      ],
      "metadata": {
        "id": "ECuIe41Iet7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def salvar_csv(dataframe: pd.DataFrame, nome_arquivo: str):\n",
        "\n",
        "    try:\n",
        "        # index=False evita que o índice do pandas seja salvo como uma coluna no csv\n",
        "        # encoding='utf-8-sig' garante compatibilidade com excel para caracteres especiais\n",
        "        dataframe.to_csv(nome_arquivo, index=False, encoding='utf-8-sig')\n",
        "        print(f\"dataframe salvo com sucesso em '{nome_arquivo}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"erro ao salvar o arquivo: {e}\")"
      ],
      "metadata": {
        "id": "tldoD8YJeqrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execução principal que chama as outras funções."
      ],
      "metadata": {
        "id": "DBLU5CMkmTvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # definição dos arquivos de origem\n",
        "    ARQUIVO_SPEEDRUN = 'speedrun_stats.csv'\n",
        "    ARQUIVO_TWITCH = 'twitch_stats.csv'\n",
        "    ARQUIVO_YOUTUBE = 'youtube_stats.csv'\n",
        "    ARQUIVO_BILIBILI = 'bilibili_stats.csv'\n",
        "    ARQUIVO_SAIDA = 'dados_analise_final.csv'\n",
        "\n",
        "    # unificação e limpeza\n",
        "    stats_unificados = unificar_estatisticas_videos(ARQUIVO_TWITCH, ARQUIVO_YOUTUBE, ARQUIVO_BILIBILI)\n",
        "    dados_completos = juntar_e_limpar_dados(ARQUIVO_SPEEDRUN, stats_unificados)\n",
        "\n",
        "    # processamento e limpeza dos dados\n",
        "    stats_unificados = unificar_estatisticas_videos(ARQUIVO_TWITCH, ARQUIVO_YOUTUBE, ARQUIVO_BILIBILI)\n",
        "    dados_completos = juntar_e_limpar_dados(ARQUIVO_SPEEDRUN, stats_unificados)\n",
        "\n",
        "    # filtro para remover runs sem vídeo associado (onde 'plataforma' é -1)\n",
        "    df_analise_filtrado = dados_completos[dados_completos['plataforma'] != -1].copy()\n",
        "\n",
        "    # seleção das métricas relevantes para análise\n",
        "    colunas_relevantes = [\n",
        "        'run_date', 'run_player', 'run_time_seconds', 'video_link', 'plataforma',\n",
        "        'video_data_publicacao', 'VideoAgeDays', 'video_views',\n",
        "        'video_likes', 'video_comentarios', 'ViewsPerDay', 'EngagementRate',\n",
        "        'ChannelID', 'CurrentSubscribers', 'Views_5d_Depois', 'Likes_5d_Depois',\n",
        "        'danmaku', 'coins', 'shares', 'favorites'\n",
        "    ]\n",
        "\n",
        "    # filtra o dataframe completo para manter apenas colunas relevantes\n",
        "    df_analise = df_analise_filtrado.reindex(columns=colunas_relevantes, fill_value=-1)\n",
        "\n",
        "    # exibição e análise dos resultados\n",
        "    print(\" Dataframe de análise (apenas colunas relevantes, runs sem vídeo removidas):\")\n",
        "    print(df_analise.head(10))\n",
        "\n",
        "    print(\"\\n Informações e tipos de dados do dataframe de análise:\")\n",
        "    df_analise.info()\n",
        "\n",
        "    print(\"\\n Exemplo de runs do twitch no dataframe limpo:\")\n",
        "    if not df_analise[df_analise['plataforma'] == 'Twitch'].empty:\n",
        "        print(df_analise[df_analise['plataforma'] == 'Twitch'].head())\n",
        "    else:\n",
        "        print(\"\\n Nenhuma run do twitch encontrada após a filtragem.\")\n",
        "\n",
        "\n",
        "    print(\"\\n Exemplo de runs do youtube no dataframe limpo:\")\n",
        "    print(df_analise[df_analise['plataforma'] == 'YouTube'].head())\n",
        "\n",
        "    print(\"\\n Exemplo de runs do bilibili no dataframe limpo:\")\n",
        "    print(df_analise[df_analise['plataforma'] == 'Bilibili'].head())\n",
        "\n",
        "    print(\"\\n Resumo estatístico para identificar outliers:\")\n",
        "    colunas_numericas = df_analise.select_dtypes(include=np.number).columns\n",
        "    print(df_analise[colunas_numericas].describe().apply(lambda s: s.apply('{0:.2f}'.format)))\n",
        "\n",
        "    print(\"\\n--- Salvando resultado ---\")\n",
        "    salvar_csv(df_analise, ARQUIVO_SAIDA)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtlKmaJl0gt2",
        "outputId": "02e06d85-bae9-4ea5-c5e6-db1f71f2748b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processando Bilibili a partir de: bilibili_stats.csv ---\n",
            "Bilibili: CSV carregado. 10 linhas iniciais.\n",
            "Bilibili: 2 vídeos com 'context_video' == 'recorde'.\n",
            "Bilibili: 2 recordes processados e adicionados ao DataFrame.\n",
            "\n",
            "--- Processando Bilibili a partir de: bilibili_stats.csv ---\n",
            "Bilibili: CSV carregado. 10 linhas iniciais.\n",
            "Bilibili: 2 vídeos com 'context_video' == 'recorde'.\n",
            "Bilibili: 2 recordes processados e adicionados ao DataFrame.\n",
            " Dataframe de análise (apenas colunas relevantes, runs sem vídeo removidas):\n",
            "     run_date run_player  run_time_seconds                               video_link plataforma      video_data_publicacao  VideoAgeDays  video_views  video_likes  video_comentarios  ViewsPerDay  EngagementRate                 ChannelID  CurrentSubscribers  Views_5d_Depois  Likes_5d_Depois  danmaku  coins  shares  favorites\n",
            "3  2022-06-25   1xygz7wx            3979.0  https://youtube.com/watch?v=7vycum--l3g    YouTube  2022-06-25 00:18:57+00:00        1202.0       1114.0         25.0                3.0     0.926789        2.513465  UCjiL8i_nmBNuCTO28luuoew               928.0              0.0              0.0     -1.0   -1.0    -1.0       -1.0\n",
            "4  2022-07-02   68wq1z8g            3908.0  https://youtube.com/watch?v=dgetv7usduq    YouTube  2022-07-02 20:52:35+00:00        1194.0       5098.0        141.0               27.0     4.269682        3.295410  UC1FMaP1PwqIx7houyjJF9tg             12100.0           7534.0            283.0     -1.0   -1.0    -1.0       -1.0\n",
            "5  2022-07-04   68wq1z8g            3880.0  https://youtube.com/watch?v=ss1sebramrw    YouTube  2022-07-04 14:33:03+00:00        1192.0       2122.0         92.0               19.0     1.780201        5.230914  UC1FMaP1PwqIx7houyjJF9tg             12100.0           5412.0            191.0     -1.0   -1.0    -1.0       -1.0\n",
            "6  2022-07-05   68wq1z8g            3855.0  https://youtube.com/watch?v=s4mqvuwmr2c    YouTube  2022-07-06 10:11:08+00:00        1191.0       1215.0         47.0               12.0     1.020151        4.855967  UC1FMaP1PwqIx7houyjJF9tg             12100.0           4197.0            144.0     -1.0   -1.0    -1.0       -1.0\n",
            "7  2022-07-06   68wq1z8g            3804.0  https://youtube.com/watch?v=8mmypyyrz4s    YouTube  2022-07-07 09:36:14+00:00        1190.0       2276.0         75.0               19.0     1.912605        4.130053  UC1FMaP1PwqIx7houyjJF9tg             12100.0           1629.0             40.0     -1.0   -1.0    -1.0       -1.0\n",
            "8  2022-07-12   68wq1z8g            3793.0  https://youtube.com/watch?v=jwcrkur7_cy    YouTube  2022-07-12 08:47:49+00:00        1185.0       1629.0         40.0               17.0     1.374684        3.499079  UC1FMaP1PwqIx7houyjJF9tg             12100.0           1903.0             69.0     -1.0   -1.0    -1.0       -1.0\n",
            "9  2022-07-14   68wq1z8g            3782.0  https://youtube.com/watch?v=2g7m9no2egu    YouTube  2022-07-15 10:17:29+00:00        1182.0       1903.0         69.0               11.0     1.609983        4.203889  UC1FMaP1PwqIx7houyjJF9tg             12100.0           4368.0            116.0     -1.0   -1.0    -1.0       -1.0\n",
            "10 2022-07-20   68wq1z8g            3711.0  https://youtube.com/watch?v=nrqmpypcwnk    YouTube  2022-07-21 09:40:57+00:00        1176.0       8048.0        170.0               31.0     6.843537        2.497515  UC1FMaP1PwqIx7houyjJF9tg             12100.0              0.0              0.0     -1.0   -1.0    -1.0       -1.0\n",
            "11 2022-08-02   68wq1z8g            3649.0  https://youtube.com/watch?v=3idfym9rs44    YouTube  2022-08-02 16:11:27+00:00        1163.0       6095.0        190.0               37.0     5.240757        3.724364  UC1FMaP1PwqIx7houyjJF9tg             12100.0          16550.0            310.0     -1.0   -1.0    -1.0       -1.0\n",
            "12 2022-08-05   68wq1z8g            3620.0  https://youtube.com/watch?v=mqba2tvpnb4    YouTube  2022-08-06 14:06:18+00:00        1159.0      16550.0        310.0               52.0    14.279551        2.187311  UC1FMaP1PwqIx7houyjJF9tg             12100.0              0.0              0.0     -1.0   -1.0    -1.0       -1.0\n",
            "\n",
            " Informações e tipos de dados do dataframe de análise:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 36 entries, 3 to 38\n",
            "Data columns (total 20 columns):\n",
            " #   Column                 Non-Null Count  Dtype         \n",
            "---  ------                 --------------  -----         \n",
            " 0   run_date               36 non-null     datetime64[ns]\n",
            " 1   run_player             36 non-null     object        \n",
            " 2   run_time_seconds       36 non-null     float64       \n",
            " 3   video_link             36 non-null     object        \n",
            " 4   plataforma             36 non-null     object        \n",
            " 5   video_data_publicacao  36 non-null     object        \n",
            " 6   VideoAgeDays           36 non-null     float64       \n",
            " 7   video_views            36 non-null     float64       \n",
            " 8   video_likes            36 non-null     float64       \n",
            " 9   video_comentarios      36 non-null     float64       \n",
            " 10  ViewsPerDay            36 non-null     float64       \n",
            " 11  EngagementRate         36 non-null     float64       \n",
            " 12  ChannelID              36 non-null     object        \n",
            " 13  CurrentSubscribers     36 non-null     float64       \n",
            " 14  Views_5d_Depois        36 non-null     float64       \n",
            " 15  Likes_5d_Depois        36 non-null     float64       \n",
            " 16  danmaku                36 non-null     float64       \n",
            " 17  coins                  36 non-null     float64       \n",
            " 18  shares                 36 non-null     float64       \n",
            " 19  favorites              36 non-null     float64       \n",
            "dtypes: datetime64[ns](1), float64(14), object(5)\n",
            "memory usage: 5.9+ KB\n",
            "\n",
            " Exemplo de runs do twitch no dataframe limpo:\n",
            "nNenhuma run do twitch encontrada após a filtragem.\n",
            "\n",
            " Exemplo de runs do youtube no dataframe limpo:\n",
            "    run_date run_player  run_time_seconds                               video_link plataforma      video_data_publicacao  VideoAgeDays  video_views  video_likes  video_comentarios  ViewsPerDay  EngagementRate                 ChannelID  CurrentSubscribers  Views_5d_Depois  Likes_5d_Depois  danmaku  coins  shares  favorites\n",
            "3 2022-06-25   1xygz7wx            3979.0  https://youtube.com/watch?v=7vycum--l3g    YouTube  2022-06-25 00:18:57+00:00        1202.0       1114.0         25.0                3.0     0.926789        2.513465  UCjiL8i_nmBNuCTO28luuoew               928.0              0.0              0.0     -1.0   -1.0    -1.0       -1.0\n",
            "4 2022-07-02   68wq1z8g            3908.0  https://youtube.com/watch?v=dgetv7usduq    YouTube  2022-07-02 20:52:35+00:00        1194.0       5098.0        141.0               27.0     4.269682        3.295410  UC1FMaP1PwqIx7houyjJF9tg             12100.0           7534.0            283.0     -1.0   -1.0    -1.0       -1.0\n",
            "5 2022-07-04   68wq1z8g            3880.0  https://youtube.com/watch?v=ss1sebramrw    YouTube  2022-07-04 14:33:03+00:00        1192.0       2122.0         92.0               19.0     1.780201        5.230914  UC1FMaP1PwqIx7houyjJF9tg             12100.0           5412.0            191.0     -1.0   -1.0    -1.0       -1.0\n",
            "6 2022-07-05   68wq1z8g            3855.0  https://youtube.com/watch?v=s4mqvuwmr2c    YouTube  2022-07-06 10:11:08+00:00        1191.0       1215.0         47.0               12.0     1.020151        4.855967  UC1FMaP1PwqIx7houyjJF9tg             12100.0           4197.0            144.0     -1.0   -1.0    -1.0       -1.0\n",
            "7 2022-07-06   68wq1z8g            3804.0  https://youtube.com/watch?v=8mmypyyrz4s    YouTube  2022-07-07 09:36:14+00:00        1190.0       2276.0         75.0               19.0     1.912605        4.130053  UC1FMaP1PwqIx7houyjJF9tg             12100.0           1629.0             40.0     -1.0   -1.0    -1.0       -1.0\n",
            "\n",
            " Exemplo de runs do bilibili no dataframe limpo:\n",
            "     run_date run_player  run_time_seconds                                   video_link plataforma video_data_publicacao  VideoAgeDays  video_views  video_likes  video_comentarios  ViewsPerDay  EngagementRate ChannelID  CurrentSubscribers  Views_5d_Depois  Likes_5d_Depois  danmaku   coins  shares  favorites\n",
            "37 2024-11-23   xk1lq12j            3237.0  https://www.bilibili.com/video/bv1znbbybeen   Bilibili   2024-11-22 00:00:00          -1.0     237288.0       7962.0              762.0         -1.0            -1.0        -1                -1.0             -1.0             -1.0   2309.0  4165.0  1025.0     2257.0\n",
            "38 2025-03-22   xk1lq12j            3175.0  https://www.bilibili.com/video/bv1ooxwyier2   Bilibili   2025-03-22 00:00:00          -1.0     313585.0       7262.0              607.0         -1.0            -1.0        -1                -1.0             -1.0             -1.0   2452.0  2764.0  1208.0     2349.0\n",
            "\n",
            " Resumo estatístico para identificar outliers:\n",
            "      run_time_seconds VideoAgeDays video_views video_likes video_comentarios ViewsPerDay EngagementRate CurrentSubscribers Views_5d_Depois Likes_5d_Depois  danmaku    coins   shares favorites\n",
            "count            36.00        36.00       36.00       36.00             36.00       36.00          36.00              36.00           36.00           36.00    36.00    36.00    36.00     36.00\n",
            "mean           3521.83       933.61   114950.31     1975.06            141.11      119.13           2.51           36634.06        15345.31          222.81   131.31   191.53    61.08    127.00\n",
            "std             210.14       300.98   269443.03     4247.87            226.75      306.43           1.32          109365.80        72068.28          843.56   553.51   822.30   260.53    535.36\n",
            "min            3175.00        -1.00     1114.00       25.00              3.00       -1.00          -1.00              -1.00           -1.00           -1.00    -1.00    -1.00    -1.00     -1.00\n",
            "25%            3346.50       824.50     5538.25      138.75             25.75        3.90           1.85           12100.00            0.00            0.00    -1.00    -1.00    -1.00     -1.00\n",
            "50%            3503.50       951.50    14327.50      305.50             45.00       11.65           2.55           12100.00            0.00            0.00    -1.00    -1.00    -1.00     -1.00\n",
            "75%            3627.25      1160.00    75070.75     1118.75            123.25       34.28           3.35           12100.00         5449.00          150.50    -1.00    -1.00    -1.00     -1.00\n",
            "max            3979.00      1202.00  1490136.00    22006.00            932.00     1597.14           5.23          481000.00       434427.00         5095.00  2452.00  4165.00  1208.00   2349.00\n",
            "\n",
            "--- Salvando resultado ---\n",
            "dataframe salvo com sucesso em 'dados_analise_final.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3019352523.py:19: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  df_final['video_data_publicacao'] = pd.to_datetime(df_final['video_data_publicacao'], errors='coerce')\n",
            "/tmp/ipython-input-3019352523.py:19: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  df_final['video_data_publicacao'] = pd.to_datetime(df_final['video_data_publicacao'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMENTÁRIO DA EQUIPE:\n",
        "O scraping de speedrun.py por motivos tecnicos acabou não gerando os mesmos links utilizados no twitch_coleta_dados.py, portanto infelizmente foi necessário excluir os dados dessa plataforma."
      ],
      "metadata": {
        "id": "-BTtqt4pdT_A"
      }
    }
  ]
}